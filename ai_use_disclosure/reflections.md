Q1. There are many risks using GenAI poses. The primary concern in my opinion would be correctness and reliablity. GenAI models can generate plausible-looking code that seems fine and correct but in reality could be wrong. 

This can lead to a false sense of security by the developer, causing them to overlook deeper flaws. Related to this problem, is the generation of tests. Manually creating tests takes a lot of time and effort, leading many to use AI to generate automated tests. The problem with this is that AI may generate tests based off the implementation so your code may pass the AI-generated tests but in fact, it may fail some other tests. 

Q2. I treated GenAI as a suggestion not as a final solution. I would also go through each block of code carefully, checking for logical correctness and seeing if I can understand the process and why it happens. 

When I would use AI to generate tests, I would employ the same method when I checked its implemnetation. I would go through the tests and see if they were correct. In addition, this may be a slightly inaccurate/unreliable method, I would also use different AI models e.g. Google Gemini, Claude to go through the tests and implementation to see their thoughts and insights. I find that other AI is also able to pick up on subtle mistakes/incorrect code quite accurately. 
